{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "621af092",
   "metadata": {},
   "source": [
    "Climate resiliance and risk managment Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d072b3",
   "metadata": {},
   "source": [
    "Installation of pacjages that will be needed for this model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70543494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63696cd7",
   "metadata": {},
   "source": [
    "Class for all Risk assessment Models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35706e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRiskModel:\n",
    "    def __init__(self, model_name, model_type='classification'):\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_selector = None\n",
    "        self.is_trained = False\n",
    "        self.features = None\n",
    "        self.training_history = []\n",
    "        self.model_params = {}\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        \"\"\"Prepare data for model training/prediction\"\"\"\n",
    "        raise NotImplementedError(\"Must implement prepare_data method\")   \n",
    "    \n",
    "    def train(self, X, y):\n",
    "        \"\"\"Train the risk model\"\"\"\n",
    "        raise NotImplementedError(\"Must implement train method\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make risk predictions\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise ValueError(\"Model must be trained before making predictions\")\n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            return self.model.predict_proba(X)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate model performance\"\"\"\n",
    "        predictions = self.predict(X_test)\n",
    "        probabilities = self.predict_proba(X_test)\n",
    "        \n",
    "        evaluation = {\n",
    "            'accuracy': accuracy_score(y_test, predictions),\n",
    "            'classification_report': classification_report(y_test, predictions),\n",
    "            'confusion_matrix': confusion_matrix(y_test, predictions).tolist(),\n",
    "            'model_name': self.model_name,\n",
    "            'test_samples': len(X_test),\n",
    "            'evaluation_date': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Add AUC score if probabilities available\n",
    "        if probabilities is not None and len(np.unique(y_test)) == 2:\n",
    "            evaluation['auc_score'] = roc_auc_score(y_test, probabilities[:, 1])\n",
    "        \n",
    "        return evaluation\n",
    "    \n",
    "    def save_model(self, filepath):\n",
    "        \"\"\"Save trained model to file\"\"\"\n",
    "        model_data = {\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_selector': self.feature_selector,\n",
    "            'model_name': self.model_name,\n",
    "            'model_type': self.model_type,\n",
    "            'features': self.features,\n",
    "            'is_trained': self.is_trained,\n",
    "            'training_history': self.training_history,\n",
    "            'model_params': self.model_params,\n",
    "            'save_date': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        filepath = Path(filepath)\n",
    "        if filepath.suffix == '.pkl':\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(model_data, f)\n",
    "        elif filepath.suffix == '.joblib':\n",
    "            joblib.dump(model_data, filepath)\n",
    "        \n",
    "        print(f\"Model saved to: {filepath}\")\n",
    "    \n",
    "    def load_model(self, filepath):\n",
    "        \"\"\"Load trained model from file\"\"\"\n",
    "        filepath = Path(filepath)\n",
    "        if filepath.suffix == '.pkl':\n",
    "            with open(filepath, 'rb') as f:\n",
    "                model_data = pickle.load(f)\n",
    "        elif filepath.suffix == '.joblib':\n",
    "            model_data = joblib.load(filepath)\n",
    "        \n",
    "        self.model = model_data['model']\n",
    "        self.scaler = model_data.get('scaler', StandardScaler())\n",
    "        self.feature_selector = model_data.get('feature_selector', None)\n",
    "        self.model_name = model_data['model_name']\n",
    "        self.model_type = model_data['model_type']\n",
    "        self.features = model_data['features']\n",
    "        self.is_trained = model_data['is_trained']\n",
    "        self.training_history = model_data.get('training_history', [])\n",
    "        self.model_params = model_data.get('model_params', {})\n",
    "        \n",
    "        print(f\"Model loaded from: {filepath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b087b5",
   "metadata": {},
   "source": [
    "LANDSLIDE RISK MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3344dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RwandaLandslideModel():\n",
    "    \"\"\"Advanced landslide risk prediction model for Rwanda\"\"\"\n",
    "\n",
    "    def __init__(self, model_algorithm ='random_forest'):\n",
    "\n",
    "        super().__init__(\"Rwanda_Landslide_Model\", \"classificattion\")\n",
    "        self.algorithm = model_algorithm\n",
    "        self.rwanda_params = self.__setup_rwanda_parameters()\n",
    "        self._initialize_model()\n",
    "\n",
    "    def __setup_rwanda_parameters(self):\n",
    "        \"\"\"setup Rwanda specific landslide parameters\"\"\"\n",
    "        return {\n",
    "            'elevation_range': (900, 4507), #Rwanda's elevation range (m)\n",
    "            'critical_slope': 25, #Critical slope angle (degrees)\n",
    "            'rainfall_threshold': 50, #Critical 24-hour rainfall (mm)\n",
    "            'soil_saturation_threshold': 0.8, #Critical soil moisture\n",
    "            'vegetation_threshold': 0.3, #Critical NDVI threshold\n",
    "            'geolgy_weights': {\n",
    "                'volcanic': 0.6, #More stable\n",
    "                'sedimentary': 0.3, #Moderate stability\n",
    "                'weathered': 1.0, #Less stable\n",
    "                'alluvial': 0.7 #Variable stability\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _initialize_model(self):\n",
    "        \"\"\" Initialize the underlying ML model \"\"\"\n",
    "        if self.algorithm == 'random_forest':\n",
    "            self.model = RandomForestClassifier(\n",
    "                n_estimators= 200,\n",
    "                max_depth= 15,\n",
    "                min_samples_split= 10,\n",
    "                min_samples_leaf=5,\n",
    "                random_state= 42,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "        elif self.algorithm == 'gradient_boosting':\n",
    "            self.model = GradientBoostingClassifier(\n",
    "                n_estimators= 150,\n",
    "                learning_rate= 0.1,\n",
    "                max_depth= 8,\n",
    "                random_state= 42            \n",
    "                )\n",
    "        elif self.algorithm == 'Logistic_Regression':\n",
    "            self.model = LogisticRegression( \n",
    "                random_state= 42,\n",
    "                class_weight='balanced',\n",
    "                max_iter= 1000\n",
    "            )\n",
    "    \n",
    "    def generate_training_data(self, n_samples= 5000):\n",
    "        \"\"\"Generate realistic training data for Rwanda landslidies\"\"\"\n",
    "        print(f\"Generating {n_samples} training samples for landslide model...\")\n",
    "        \n",
    "        np.random.seed(42)\n",
    "        data = []\n",
    "\n",
    "        # Rwanda's geographic regions with different landslide susceptibility\n",
    "        regions = {\n",
    "            'Northern_Mountains': {'base_risk': 0.7, 'elevation_mean': 2200, 'slope_mean': 30},\n",
    "            'Central_Plateau': {'base_risk': 0.4, 'elevation_mean': 1600, 'slope_mean': 20},\n",
    "            'Eastern_Hills': {'base_risk': 0.5, 'elevation_mean': 1400, 'slope_mean': 25},\n",
    "            'Western_Ridges': {'base_risk': 0.6, 'elevation_mean': 1800, 'slope_mean': 28},\n",
    "            'Southern_Valleys': {'base_risk': 0.3, 'elevation_mean': 1300, 'slope_mean': 15}\n",
    "        }\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            #Select random region\n",
    "\n",
    "            region_name, region_params = np.random.choice(list(regions.items()))\n",
    "\n",
    "            #Generate geographic features\n",
    "            elevation = np.random.normal(region_params['elevation_mean'], 200)\n",
    "            elevation = np.clip(elevation, 900, 4500)\n",
    "            slope = np.random.normal(region_params['slope_mean'], 8)\n",
    "            slope = np.clip(slope, 0, 60)\n",
    "\n",
    "            aspect = np.random.uniform(0,360)\n",
    "\n",
    "            #Generate climate features\n",
    "            #Rainfall patterns (more in wet season)\n",
    "\n",
    "            season_factor = np.sin((i % 365) * (2 * np.pi / 365) * 0.3 + 1)\n",
    "            daily_rainfall = np.random.gamma(2, 3) * season_factor\n",
    "            rainfall_24h = daily_rainfall + np.random.gamma(1, 10) #Extrem events\n",
    "            rainfall_72h =  rainfall_24h + np.random.gamma(1, 15)\n",
    "            antecedent_rainfall = np.random.gamma(3, 8) #previous week rainfall\n",
    "\n",
    "            # Soil and vegetation features\n",
    "            soil_depth = np.ranndom.gamma(2,1.5) #meters\n",
    "            soil_type = np.random.choice(['volcanic', 'sedimentary', 'weathered', 'alluvial'])\n",
    "            soil_moisture = 0.3 + 0.4 * (rainfall_24h / 100) + np.random.normal(0,0.1)\n",
    "            soil_moisture = np.clip(soil_moisture, 0.1, 0.95)\n",
    "\n",
    "            #Human factors\n",
    "            \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
